knnmatrix[i].nonzero()[1][1:]  # nearest neighbors(k) for i'th element in matrix

%% final submission %%
- bagging of different xgboost: 
	- randomly picking one of the versions(svm-light)
	- varying the params xgb (picking randomly from small set like- 0.7,0.8,0.9)
	- random seed each time
	- atleast 20!! (one will be the best, rest randomly choosen)
	- maybe have little less eta and more rounds ?

IMP**- top f-score features from XGB, use them for NN	
	
IMP**- UPC features knn and distribute weight to neigbours
	
IMP**- use labels generated from 20 + current best to psuedo-label the test set.. than train on train+test.. and run on test again(with the current best of 300)
	
- next submissions-- single best out of 20(lowest train loss), all 20 average, average of current best + (20 average), average 21(20 + current best), 10 best out of 20

IMP-- try setting results lower than 5th/7th prediction to very low value (not zero). Have a threshold. subtract from all values(except top 5) that threshold and keep a sum. After that use that sum value, and distribute it over the top 5 entries in the same ratio as they are!!!

- UPC corellation matrix and distribute weight when knn=4 (read forum post again)

** CURRENT TARGET ON LB **
> log(0.4965853) [1] (-0.7)
	- Need average prob to be 0.5
	- Reduce absolute 1e-15 penalty(there are no 1e-5 penalties in train)

--->>> be conservative while adding unique len feature OR when length is small (like 2/3), less feature pollution

--->>> add multiples thing
--->>> class imblance (set cutoff point), and try to do ensemble with multiple xgb's 
--->>> add UPC feature or maybe its co-rellation feature ?
--->>> add DD+FLN number as combined feature(will be 10K unique) !!!	
-->> xgb-model-importance
- TF-IDF (bad)
- sub-sample (good!)

%% v5.1 changes %%
<COSINE SIMILARITY-DISTANCE>(Theory: things that are bought together, in one VisitNumber. so a DxD matrix between DepartmentDescriptions storing their relative "closeness"(distance-k). So init with ZERO/NA, and as the DD's are purchased together for any VN, update their EDGE score(matrix_ij/_ji weight). This weight decreses as the number of iterms bought together in one VN increases (decrease in exclusiveness). This will also HELP WITH OUTLIER BUYS!!. Basically want to find the general trend of buys a customer makes. After matrix is made, invert weights to make a cost-edge graph. So lesser the cost, more closer the items)
(for k=1,3)
- [1]-as stats: For each VN, use the cumulative(mod/squares) distance of all DD's present as one feature (for 5 DD'd there are 10 interaction pairs->can USE STATS). This will enable us to include cases where buys are local(last minute shopping for specific items) or buys are distributed(weekend shopping) (CosineDistance Stats- pairwise for each edge, so for 4 DD's, 6 possible edge pairs to get CosineDistance from!)
- [2]:using number of edges between DD's, for some (K)nn: Count of no. of edges for each DD pair. Use stats agai ( LENGTH -1, because first is always the vertex itself )

## v5.2 changes %%
- some small changes closely related to v5.1

## v5.3 changes %%
- added UPC selective features (only for UPC10,UPC11; for rest as it is)

## v5.4 changes %%
(I)- only considering columns that are in train (need to do this for UPC similarity matrix), this will also remove clutter
(II)- distributed weights to UPC neighbors(k=3), features became less sparse

## v5.5 changes %%
- removed FLN from features (currently at 11k, will be 6k afterwards)

## v5.x changes %%
- NO proper way to assess pairwise DD for case where multiple items were bought from the same DD(try using multiples for now)

## v5.x changes %%
- square all values before applying stats 

%% v5.x changes %%
- Same as v5.1, but after removing removing FLN features

%% v5.x changes %%
- set non present values to some negative number, or very high positive number (currently zero)
%% v5.x changes %%
- add cosine weights directly also (without any k-nn stuff)

%% v5.x changes %%
- [3]-as clusters: try to cluster them, then have a feature telling which cluster the buys belong too. Output can be multiple clusters, so can use those STATS again!
- [4]: instead of having sparse 5000 features, can either diffuse and make dense features OR use less features (point 2 -clusters)
([Advance Feature-gist of above ideas] try to cluster dept based on TripType(or something), find a metric to calculate "distance" between cluster points. Try to include this distance metric for every VisitNumber to aid TripType classification !!!)

%% v5.x changes %%
- using the advance(sp&spl) for FLN adjacency matrix.

===
- Sparse Cluster and its count, Cluster stats (for simple cluster)


=**=> should make same co-sine similarity for <FineLineNumber>!!! 

%% v6 changes %%
- add digit-wise UPC features (max-12)

%% v7 changes %%
-> remove the multiple FLN(5000+) features after adding above changes
%% v8 changes %%
-> remove low frequency classes from train (need to confirm)
-> balanced classes (limit to 3600 max records per classes, may have to make 3 ensembles)


%% v4.1 changes %%
-^ BIGGEST TODO: combine categories - MENSWEAR and MENS WEAR
-^ drop extra feats for -ve DD !?!? => -DD features
-^ do all those 4 STATS features above on sum(SC) for each DD group, and EACH FLN group => +4*2 features
-^ MAX SC and its corresponding dept/fln/upc ? 
	-^ DD's with top two SC; Cond: SC>1, and not(SC2<SC1/2) => +2 features

%% v4.2 changes %% [BAD!!][archived-changes]#reversed
- REMOVED FLN 5k features!!!!

%% v4.3 changes %%	
- add UpcLenght as feature(IMPORTANT!!)
	- how handle missing Upc(NAs? which are: DD{Pharmacy,Null} 50%TT5)
	- Also, how to compress ?
		-^ maybe use stats feature similar to scan count(Good- SD,skew,kurt) UpcLen example: [11,10,11,11,8,12] 
		- freq count of upcLen per VN), 
Train/Test: unique upcLen- 2 3 4 5 7 8 9 10 11 12

%% v4.x changes %%	[FOR FUTURE]
==+==>> UPC features (see below for details)
- 11 digit(1(property)+5(manf.code)+5(prod.code))
> length(tr11$Upcl5) [1] 168418
> length(unique(tr11$Upc))[1] 38839
> length(unique(tr11$Upc1)) [1] 7 * <- use this one
> length(unique(tr11$Upc5)) [1] 3043 * <- use this one
> length(unique(tr11$Upcl5)) [1] 28426

- 10 digit(5(manf.code)+5(prod.code)) 
> length(unique(tr10$Upc)) [1] 54143
> length(unique(tr10$Upc5)) [1] 2549 * <- use this one (maybe merge tr11$Upc5- common dict index)
> length(unique(tr10$Upcl5)) [1] 36885

- 12 digit(2,5,5) (!! only first 6 digits, last 6 ALL ZERO) <- use as it is
length(unique(tr12$Upc)) 177
> length(unique(tr12$Upc2))[1] 24 *
> length(unique(tr12$Upc5))[1] 176 *
> length((tr12$Upc5))[1] 8463

- 9 digit, Total=2166/Unique=335 (add sparse features for unique) OR <- use as it is
(4(prod)+5) 
>length(unique(tr9$Upc4)):31 *

- 8 digit, Total=412/Unique=173 (add sparse features for unique) OR <- use as it is
(3(prod)[Unique=12] *+5[Unique=172])
>length(unique(tr8$Upc3)):12 * [useless, UniqueDD=13]

- 5 digit, Total=372/Unique=43/Unique3=10 * (add sparse features for unique) <- use as it is
- 4 digit, Total=29745/Unique=207 * (add sparse features for unique) <- use as it is

- 3 digit, Total=7/Unique=1 * (just use UPC length) <- use as it is









=**=> weighted(using cv) ensemble of rf, nn, xgb
=**=>> to try- have eta>0.2 and rounds<50, and ensemble 4-5 xgb classifications with random seed values	

=***=>> custom cross_entropy function, scatter plot of 'confused' classes with HIGH(1e-15) mistake
-->[FAIL!!] checking binary classification accuracy for 'confused' classes, and maybe use those to ensemble with xgb. So when binary classifier predicts a class, set that to ONE, and rest to ZERO, and do ensemble average with XGB.

-->[not total FAIL!!] max_delta_step=1,5,10 (supposedly to help with skewed classes)

=***

[Done]=====> add records=.N(purchases)[good], uniqDD(varying purchases), Sum, MAX, Standard Deviation, skewness, kurtosis of SC (USE IT!!, good feature acc to graph) (just add all maybe,#YOLO) {trvn <- train[,.(uniqDD = length(unique(DepartmentDescription)),tt=unique(TripType),records=.N),VisitNumber]}




[Done]-> change svmlight add extra for negatives(v2)
-> try 0/1 categories instead of count(for rf,gnb,mnb)
-> remove weekday!! -> remove NULL category from encoding
-> low prob, med prob, high prob -
-> Dealing with SKEWED classes-Limit the Over-Represented Class

> log(1/38) [1] -3.637586 - all ones penalty, Average Penalty
> log(1e-15) [1] -34.53878 - maximum penalty!!!!AVOID!!!!

- instead of directly outputting decimal probability for all 38 classes, select top 8/4/2 and only output for them. This will reduce the overall error, because the 38 prob values are scaled to sum to 1, so unnecesary noise(classes that are not very likely- above 8/4/2) will increase the cost error for that one prob which was actually true!

====Features====
-[records]- records=.N

-[Weekday]- Weekday ? (maybe)

-[UPC] - can be used to count no. of items bought (for now)
- shouldn't use UPC directly, way to many!!, try to extract out features. OR can make different models for short length UPC, or diff models for each UPC, BUT a visit number can have short and long UPC too!!


-[DD+SC]- using (DepartmentDescription+ScanCount) to generate features: 69 (for each dept.train&test)x2(count of positves&negatives ScanCount separately)

-[FLN+SC]- Should use it directly(!?), has ~5000 unique values, will provide for fine-grain classification.





test.csv.zip password - Work4WalmarT